{"file_contents":{"ast_nodes.py":{"content":"# ast_nodes.py\n\nclass NumberNode:\n    def __init__(self, token):\n        self.token = token\n\n    def __repr__(self):\n        return f'{self.token.value}'\n\nclass StringNode:\n    def __init__(self, token):\n        self.token = token\n\n    def __repr__(self):\n        return f'\"{self.token.value}\"'\n\nclass BinOp:\n    def __init__(self, left, op, right):\n        self.left = left\n        self.op = op\n        self.right = right\n\n    def __repr__(self):\n        return f'({self.left} {self.op.value} {self.right})'\n\nclass VarDecl:\n    def __init__(self, name, value):\n        self.name = name\n        self.value = value\n\n    def __repr__(self):\n        return f'VarDecl(name={self.name}, value={self.value})'\n\nclass VarAccess:\n    def __init__(self, name):\n        self.name = name\n\n    def __repr__(self):\n        return f'VarAccess(name={self.name})'\n\nclass IfNode:\n    def __init__(self, condition, body):\n        self.condition = condition\n        self.body = body\n\n    def __repr__(self):\n        return f'If(condition={self.condition}, body={self.body})'\n\nclass BlockNode:\n    def __init__(self, statements):\n        self.statements = statements\n\n    def __repr__(self):\n        return f'Block({self.statements})'\n\n# UnaryOp pode ser útil no futuro para operadores como '-' (negativo)\nclass UnaryOp:\n    def __init__(self, op, node):\n        self.op = op\n        self.node = node\n\n    def __repr__(self):\n        return f'({self.op.value}{self.node})'\n","size_bytes":1453},"lexer.py":{"content":"# lexer.py\n\nfrom token_class import Token\nfrom token_type import TokenType\n\nclass Lexer:\n    def __init__(self, source_code):\n        self.source = source_code\n        self.position = 0\n        self.current_char = self.source[self.position] if self.position < len(self.source) else None\n\n    def advance(self):\n        \"\"\"Avança o ponteiro de posição e atualiza o caractere atual.\"\"\"\n        self.position += 1\n        if self.position < len(self.source):\n            self.current_char = self.source[self.position]\n        else:\n            self.current_char = None # Fim do arquivo\n\n    def skip_whitespace(self):\n        \"\"\"Pula espaços em branco, tabulações e quebras de linha.\"\"\"\n        while self.current_char is not None and self.current_char.isspace():\n            self.advance()\n\n    def skip_comment(self):\n        \"\"\"Pula comentários de linha única que começam com //.\"\"\"\n        if self.current_char == '/' and self.peek() == '/':\n            while self.current_char is not None and self.current_char != '\\n':\n                self.advance()\n\n    def peek(self):\n        \"\"\"Olha o próximo caractere sem avançar a posição.\"\"\"\n        peek_pos = self.position + 1\n        if peek_pos < len(self.source):\n            return self.source[peek_pos]\n        return None\n\n    def get_identifier_or_keyword(self):\n        \"\"\"Processa um identificador ou uma palavra-chave.\"\"\"\n        result = \"\"\n        while self.current_char is not None and self.current_char.isalnum():\n            result += self.current_char\n            self.advance()\n\n        # Mapeia palavras-chave para seus tipos de token\n        keywords = {\n            \"var\": TokenType.VAR_KEYWORD,\n            \"if\": TokenType.IF_KEYWORD,\n            \"else\": TokenType.ELSE_KEYWORD,\n            # Adicione outras palavras-chave aqui (while, for, etc.)\n        }\n\n        # Se a string 'result' for uma palavra-chave, retorna o tipo correspondente.\n        # Caso contrário, é um identificador.\n        token_type = keywords.get(result, TokenType.IDENTIFIER)\n        return Token(token_type, result)\n\n    def get_number(self):\n        \"\"\"Processa um número inteiro.\"\"\"\n        result = \"\"\n        while self.current_char is not None and self.current_char.isdigit():\n            result += self.current_char\n            self.advance()\n        return Token(TokenType.INTEGER_LITERAL, int(result))\n\n    def get_string(self):\n        \"\"\"Processa uma string literal entre aspas duplas.\"\"\"\n        self.advance() # Pula a primeira aspa\n        result = \"\"\n        while self.current_char is not None and self.current_char != '\"':\n            result += self.current_char\n            self.advance()\n        self.advance() # Pula a última aspa\n        return Token(TokenType.STRING_LITERAL, result)\n\n    def get_next_token(self):\n        \"\"\"Retorna o próximo token do código-fonte.\"\"\"\n        while self.current_char is not None:\n\n            if self.current_char.isspace():\n                self.skip_whitespace()\n                continue\n\n            if self.current_char == '/' and self.peek() == '/':\n                self.skip_comment()\n                continue\n\n            if self.current_char.isalpha():\n                return self.get_identifier_or_keyword()\n\n            if self.current_char.isdigit():\n                return self.get_number()\n\n            if self.current_char == '\"':\n                return self.get_string()\n\n            # Dicionário para operadores de um caractere\n            single_char_tokens = {\n                '=': TokenType.ASSIGN,          # <-- CORREÇÃO AQUI\n                '+': TokenType.OPERATOR,\n                '<': TokenType.OPERATOR,\n                '{': TokenType.LBRACE,\n                '}': TokenType.RBRACE,\n                '(': TokenType.LPAREN,\n                ')': TokenType.RPAREN,\n                ';': TokenType.SEMICOLON,\n            }\n\n            if self.current_char in single_char_tokens:\n                token_type = single_char_tokens[self.current_char]\n                token_value = self.current_char\n                self.advance()\n                return Token(token_type, token_value)\n\n            # Se nenhum token for reconhecido, lança um erro\n            raise Exception(f\"Caractere não reconhecido: {self.current_char}\")\n\n        # Retorna o token de Fim de Arquivo (EOF) quando não há mais caracteres\n        return Token(TokenType.EOF, None)","size_bytes":4389},"main.py":{"content":"# main.py\nfrom lexer import Lexer\nfrom parser import Parser\n\ndef main():\n    try:\n        with open('codigo_fonte.lang', 'r', encoding='utf-8') as f:\n            code = f.read()\n\n        print(\"--- Iniciando Análise Léxica ---\")\n        lexer = Lexer(code)\n        tokens = lexer.tokenize()\n        print(\"Tokens Gerados:\")\n        for token in tokens:\n            print(token)\n\n        print(\"\\n--- Iniciando Análise Sintática (Parsing) ---\")\n        parser = Parser(tokens)\n        ast = parser.parse()\n        print(\"Árvore de Sintaxe Abstrata (AST) Gerada:\")\n        print(ast)\n\n    except (SyntaxError, Exception) as e:\n        print(f\"\\nOcorreu um erro durante a análise: {e}\")\n\nif __name__ == '__main__':\n    main()","size_bytes":728},"parser.py":{"content":"# parser.py\nfrom lexer import TokenType\n\n# --- Definições dos Nós da AST (Árvore de Sintaxe Abstrata) ---\n# Adicionei estas classes porque o parser precisa delas para construir a árvore.\n\nclass ASTNode:\n    pass\n\nclass ProgramNode(ASTNode):\n    def __init__(self, statements):\n        self.statements = statements\n    def __repr__(self):\n        return f\"ProgramNode({self.statements})\"\n\nclass VarDeclNode(ASTNode):\n    def __init__(self, identifier, expression):\n        self.identifier = identifier\n        self.expression = expression\n    def __repr__(self):\n        return f\"VarDeclNode(ID:{self.identifier}, Expr:{self.expression})\"\n\nclass BinOp(ASTNode):\n    def __init__(self, left, op, right):\n        self.left = left\n        self.op = op\n        self.right = right\n    def __repr__(self):\n        return f\"({self.left} {self.op.value} {self.right})\"\n\nclass NumberNode(ASTNode):\n    def __init__(self, value):\n        # Converte o valor para int aqui para garantir\n        self.value = int(value)\n    def __repr__(self):\n        return f\"Number({self.value})\"\n\nclass VarAccessNode(ASTNode):\n    def __init__(self, token):\n        self.token = token\n        self.value = token.value\n    def __repr__(self):\n        return f\"VarAccess({self.value})\"\n\n# --- Parser ---\n\nclass Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.token_idx = -1\n        self.current_token = None\n        self.advance()\n\n    def advance(self):\n        self.token_idx += 1\n        if self.token_idx < len(self.tokens):\n            self.current_token = self.tokens[self.token_idx]\n        return self.current_token\n\n    def consume(self, token_type):\n        if self.current_token.type == token_type:\n            self.advance()\n        else:\n            raise Exception(f\"Erro de sintaxe: Esperado {token_type}, mas encontrado {self.current_token.type}\")\n\n    def parse(self):\n        statements = []\n        while self.current_token.type != TokenType.EOF:\n            statements.append(self.statement())\n            # Adicionado para consumir ponto e vírgula opcionais entre as declarações\n            if self.current_token.type == TokenType.SYMBOL and self.current_token.value == ';':\n                self.consume(TokenType.SYMBOL)\n        return ProgramNode(statements)\n\n    def statement(self):\n        if self.current_token.type == TokenType.KEYWORD and self.current_token.value == 'var':\n            return self.variable_declaration()\n        else:\n            return self.expression()\n\n    def variable_declaration(self):\n        self.consume(TokenType.KEYWORD) # Consome 'var'\n        identifier_token = self.current_token\n        self.consume(TokenType.IDENTIFIER)\n        self.consume(TokenType.OPERATOR) # Consome '='\n        expr = self.expression()\n        return VarDeclNode(identifier_token.value, expr)\n\n    def expression(self):\n        return self.term()\n\n    def term(self):\n        node = self.factor()\n        while self.current_token.type == TokenType.OPERATOR and self.current_token.value in ('+', '-'):\n            op_token = self.current_token\n            self.consume(TokenType.OPERATOR)\n            right = self.factor()\n            node = BinOp(left=node, op=op_token, right=right)\n        return node\n\n    def factor(self):\n        node = self.power()\n        while self.current_token.type == TokenType.OPERATOR and self.current_token.value in ('*', '/'):\n            op_token = self.current_token\n            self.consume(TokenType.OPERATOR)\n            right = self.power()\n            node = BinOp(left=node, op=op_token, right=right)\n        return node\n\n    def power(self):\n        token = self.current_token\n\n        # >>>>> ESTA É A CORREÇÃO PRINCIPAL <<<<<\n        if token.type == TokenType.INTEGER_LITERAL:\n            self.consume(TokenType.INTEGER_LITERAL)\n            # Em vez de retornar token.value (uma string ou int), criamos um nó.\n            return NumberNode(token.value)\n\n        elif token.type == TokenType.IDENTIFIER:\n            return self.variable_access()\n\n        elif token.type == TokenType.LPAREN:\n            self.consume(TokenType.LPAREN)\n            node = self.expression()\n            self.consume(TokenType.RPAREN)\n            return node\n\n        raise Exception(f\"Fator inválido: {token}\")\n\n    def variable_access(self):\n        token = self.current_token\n        self.consume(TokenType.IDENTIFIER)\n        return VarAccessNode(token)","size_bytes":4433},"programa.c":{"content":"// Programa de exemplo para o compilador\nint main() {\n    return 0;\n}","size_bytes":69},"token_class.py":{"content":"# token_class.py\n\nclass Token:\n    \"\"\"\n    Representa um token, que é a menor unidade de código com significado.\n    Cada token tem um tipo e um valor (o texto original do código).\n    \"\"\"\n    def __init__(self, type, value):\n        self.type = type\n        self.value = value\n\n    def __repr__(self):\n        \"\"\"\n        Retorna uma representação em string do token, útil para depuração.\n        Exemplo: Token(IDENTIFIER, 'x')\n        \"\"\"\n        # self.type.name pega o nome da enumeração (ex: \"IDENTIFIER\")\n        return f\"Token({self.type.name}, '{self.value}')\"\n","size_bytes":580},"token_type.py":{"content":"# token_type.py\nfrom enum import Enum, auto\n\n# Define todos os tipos de tokens possíveis que nosso analisador léxico pode gerar.\nclass TokenType(Enum):\n    # Palavras-chave\n    VAR_KEYWORD = auto()\n    IF_KEYWORD = auto()\n    ELSE_KEYWORD = auto()\n\n    # Identificadores e Literais\n    IDENTIFIER = auto()\n    INTEGER_LITERAL = auto()\n    STRING_LITERAL = auto()\n\n    # Operadores e Pontuação\n    ASSIGN = auto()          # Para o sinal de '='\n    OPERATOR = auto()        # <-- ADICIONADO: Para '+', '-', '*', '/' etc.\n    LPAREN = auto()          # '('\n    RPAREN = auto()          # ')'\n    LBRACE = auto()          # '{'\n    RBRACE = auto()          # '}'\n    SEMICOLON = auto()       # ';'\n    COMMA = auto()           # ','\n\n    # Especiais\n    WHITESPACE = auto()\n    COMMENT = auto()\n    UNKNOWN = auto()\n    EOF = auto()             # End-Of-File (Fim do Arquivo)","size_bytes":876}},"version":1}